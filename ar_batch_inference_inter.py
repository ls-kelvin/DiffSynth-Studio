import os

os.environ["MLLM_NO_CFG"] = "1"
os.environ["TOKENIZERS_PARALLELISM"] = "false"

import argparse
import re
from pathlib import Path

import torch
from accelerate import Accelerator

from diffsynth.utils.data import save_video
from diffsynth.pipelines.wan_video_autoregressive_inter import (
    WanVideoAutoregressiveInterPipeline,
    ModelConfig,
)
from diffsynth.core.data.unified_dataset import WanVideoInterDataset


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--jsonl_path", type=str, default="/root/workspace/zzt/VideoCaption/output/agibot_result_sample.jsonl")
    parser.add_argument("--base_path", type=str, default="")
    parser.add_argument("--lora_step", type=int, default=7200)
    parser.add_argument("--run_cate", type=str, default="mllm")
    parser.add_argument("--output_dir", type=str, default=None)
    parser.add_argument("--seed", type=int, default=1)
    parser.add_argument("--fps", type=int, default=15)
    parser.add_argument("--quality", type=int, default=5)
    parser.add_argument("--disable_mllm", action="store_true", help="Disable MLLM condition (default: False)")
    parser.add_argument("--use_gt_mllm", action="store_true", help="Use input_video for MLLM context")
    parser.add_argument("--use_gt_vae", action="store_true", help="Use input_video for VAE clean latents")
    parser.add_argument("--gt_decode", action=argparse.BooleanOptionalAction, default=True, help="Decode with GT prefix latents when saving prompt-switch videos")
    parser.add_argument("--tiled", action="store_true")
    parser.add_argument("--cfg_scale", type=float, default=5.0)
    parser.add_argument("--num_inference_steps", type=int, default=50)
    parser.add_argument("--sigma_shift", type=float, default=5.0)
    parser.add_argument("--target_fps", type=int, default=6)
    parser.add_argument("--source_fps", type=int, default=30)
    parser.add_argument("--height", type=int, default=480)
    parser.add_argument("--width", type=int, default=640)
    parser.add_argument("--max_frames", type=int, default=477)
    return parser.parse_args()


def main():
    args = parse_args()
    accelerator = Accelerator()
    rank = accelerator.process_index
    world_size = accelerator.num_processes

    NEG_PROMPT = (
        "è‰²è°ƒè‰³ä¸½ï¼Œè¿‡æ›ï¼Œé™æ€ï¼Œç»†èŠ‚æ¨¡ç³Šä¸æ¸…ï¼Œå­—å¹•ï¼Œé£æ ¼ï¼Œä½œå“ï¼Œç”»ä½œï¼Œç”»é¢ï¼Œé™æ­¢ï¼Œæ•´ä½“å‘ç°ï¼Œ"
        "æœ€å·®è´¨é‡ï¼Œä½è´¨é‡ï¼ŒJPEGå‹ç¼©æ®‹ç•™ï¼Œä¸‘é™‹çš„ï¼Œæ®‹ç¼ºçš„ï¼Œå¤šä½™çš„æ‰‹æŒ‡ï¼Œç”»å¾—ä¸å¥½çš„æ‰‹éƒ¨ï¼Œ"
        "ç”»å¾—ä¸å¥½çš„è„¸éƒ¨ï¼Œç•¸å½¢çš„ï¼Œæ¯å®¹çš„ï¼Œå½¢æ€ç•¸å½¢çš„è‚¢ä½“ï¼Œæ‰‹æŒ‡èåˆï¼Œé™æ­¢ä¸åŠ¨çš„ç”»é¢ï¼Œ"
        "æ‚ä¹±çš„èƒŒæ™¯ï¼Œä¸‰æ¡è…¿ï¼ŒèƒŒæ™¯äººå¾ˆå¤šï¼Œå€’ç€èµ°"
    )

    dataset = WanVideoInterDataset(
        base_path=args.base_path,
        metadata_path=args.jsonl_path,
        target_fps=args.target_fps,
        source_fps=args.source_fps,
        height=args.height,
        width=args.width,
        num_frames=args.max_frames,
    )

    total_items = len(dataset.data) if hasattr(dataset, "data") else len(dataset)
    indices = list(range(rank, total_items, world_size))
    accelerator.print(f"[Rank {rank}] Assigned {len(indices)} / {total_items} items.")

    pipe = WanVideoAutoregressiveInterPipeline.from_pretrained(
        torch_dtype=torch.bfloat16,
        device=accelerator.device,
        model_configs=[
            ModelConfig(path="/root/workspace/zzt/models/Wan-AI/Wan2.1-T2V-1.3B/diffusion_pytorch_model.safetensors"),
            ModelConfig(path="/root/workspace/zzt/models/Wan-AI/Wan2.1-T2V-1.3B/models_t5_umt5-xxl-enc-bf16.pth"),
            ModelConfig(path="/root/workspace/zzt/models/Wan-AI/Wan2.1-T2V-1.3B/Wan2.1_VAE.pth"),
            ModelConfig(path=[
                "/root/workspace/zzt/models/Qwen/Qwen3-VL-4B-Instruct/model-00001-of-00002.safetensors",
                "/root/workspace/zzt/models/Qwen/Qwen3-VL-4B-Instruct/model-00002-of-00002.safetensors",
            ]),
        ],
        tokenizer_config=ModelConfig(path="/root/workspace/zzt/models/Wan-AI/Wan2.1-T2V-1.3B/google/umt5-xxl"),
        mllm_processor_config=ModelConfig(path="/root/workspace/zzt/models/Qwen/Qwen3-VL-4B-Instruct"),
    )

    # Load LoRA
    lora_path = f"./models/train2/Wan2.1-T2V-1.3B_lora_agibot-alpha_{args.run_cate}/step-{args.lora_step}.safetensors"
    if args.lora_step != 0:
        pipe.load_lora(pipe.dit, lora_path, alpha=1.0)
        if rank == 0:
            print(f"âœ… LoRA loaded: {lora_path}")

    output_dir = args.output_dir or f"output_videos/{args.lora_step}/{args.run_cate}"
    os.makedirs(output_dir, exist_ok=True)

    for idx in indices:
        try:
            item = dataset[idx]
        except Exception as e:
            accelerator.print(f"âŒ [Rank {rank}] Failed to load item {idx}: {e}")
            continue

        prompt_list = item.get("prompt_list", [])
        clip_frames = item.get("clip_frames", [])
        input_video = item.get("video", None)

        if not prompt_list or not clip_frames or input_video is None:
            accelerator.print(f"âŒ [Rank {rank}] Missing fields for item {idx}")
            continue

        negative_prompt_list = [NEG_PROMPT] * len(prompt_list)
        height = input_video[0].size[1]
        width = input_video[0].size[0]
        num_frames = len(input_video)

        try:
            output_video = pipe(
                prompt_list=prompt_list,
                negative_prompt_list=negative_prompt_list,
                clip_frames=clip_frames,
                input_video=input_video if (args.use_gt_mllm or args.use_gt_vae) else None,
                seed=args.seed,
                height=height,
                width=width,
                num_frames=num_frames,
                use_mllm_condition=not args.disable_mllm,
                use_gt_mllm=args.use_gt_mllm,
                use_gt_vae=args.use_gt_vae,
                gt_decode=args.gt_decode,
                cfg_scale=args.cfg_scale,
                num_inference_steps=args.num_inference_steps,
                sigma_shift=args.sigma_shift,
                tiled=args.tiled,
            )

            video_id = item["video_id"]
            filename = f"{video_id}_ar_inter.mp4"
            save_path = os.path.join(output_dir, filename)

            counter = 1
            orig_save_path = save_path
            while os.path.exists(save_path):
                name, ext = os.path.splitext(orig_save_path)
                save_path = f"{name}_{counter}{ext}"
                counter += 1

            save_video(output_video, save_path, fps=args.fps, quality=args.quality)
            accelerator.print(f"âœ… [Rank {rank}] Saved: {save_path}")

            block_videos = getattr(pipe, "last_block_videos", [])
            for block_video in block_videos:
                block_idx = block_video["block_idx"]
                prompt_idx = block_video["prompt_idx"]
                frames = block_video["frames"]
                block_filename = f"{video_id}_ar_inter_block{block_idx}_prompt{prompt_idx}.mp4"
                block_save_path = os.path.join(output_dir, block_filename)
                counter = 1
                orig_block_save_path = block_save_path
                while os.path.exists(block_save_path):
                    name, ext = os.path.splitext(orig_block_save_path)
                    block_save_path = f"{name}_{counter}{ext}"
                    counter += 1
                save_video(frames, block_save_path, fps=args.fps, quality=args.quality)
                accelerator.print(f"âœ… [Rank {rank}] Saved block-switch: {block_save_path}")

        except Exception as e:
            accelerator.print(f"âŒ [Rank {rank}] Error on item {idx}: {e}")
            continue

    accelerator.wait_for_everyone()
    if rank == 0:
        print("ğŸ‰ All done!")


if __name__ == "__main__":
    main()
