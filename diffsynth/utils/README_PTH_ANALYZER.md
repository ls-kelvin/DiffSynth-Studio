# PTH File Analyzer

This tool analyzes `.pth` files generated by the WanVideo data processing pipeline and provides detailed memory usage information for each variable.

## Features

- ðŸ” **Deep Analysis**: Recursively analyzes nested structures (dicts, lists, tuples)
- ðŸ“Š **Memory Breakdown**: Shows exact memory usage for each variable
- ðŸŽ¯ **Tensor Details**: Displays shape, dtype, device for PyTorch tensors
- ðŸ“ **Human-Readable**: Formats sizes in KB/MB/GB for easy reading
- ðŸ”„ **Batch Processing**: Analyze multiple files at once

## Usage

### Method 1: As a Python Module

```python
from diffsynth.utils.pth_analyzer import analyze_pth_file

# Analyze a single file
results = analyze_pth_file(
    'path/to/file.pth',
    max_depth=5,      # Maximum recursion depth (default: 3)
    verbose=True      # Print detailed output (default: True)
)

# Access results programmatically
print(f"Total size: {results['total_size_formatted']}")
print(f"Number of variables: {results['num_variables']}")

# Iterate through variables
for var_name, var_info in results['variables'].items():
    if var_info['type'] == 'torch.Tensor':
        print(f"{var_name}: {var_info['shape']} - {var_info['size_formatted']}")
```

### Method 2: Command Line

```bash
# Analyze a single file
python -m diffsynth.utils.pth_analyzer path/to/file.pth

# Quiet mode (suppress detailed output)
python -m diffsynth.utils.pth_analyzer path/to/file.pth --quiet

# Increase recursion depth
python -m diffsynth.utils.pth_analyzer path/to/file.pth --max-depth 10
```

### Method 3: Using Example Script

```bash
# Analyze a single file
python examples/analyze_pth_example.py path/to/file.pth

# Analyze multiple files with wildcard
python examples/analyze_pth_example.py 'data/processed/*.pth'
```

## Output Format

The analyzer provides detailed information including:

### For Tensors:
- **Type**: `torch.Tensor`
- **Shape**: Tensor dimensions
- **Dtype**: Data type (float32, int64, etc.)
- **Device**: CPU or CUDA device
- **Requires Grad**: Whether gradients are tracked
- **Size**: Memory usage in bytes and formatted

### For Other Objects:
- **Type**: Python type name
- **Size**: Memory usage in bytes and formatted
- **Preview**: Short preview of the value (for primitives)
- **Num Items**: Count of items (for collections)

## Example Output

```
================================================================================
PTH File Analysis: data/processed/sample.pth
================================================================================
Total Size: 161.85 MB (169,710,013 bytes)
Number of Variables: 5

--------------------------------------------------------------------------------
Variable Name                            Type                 Size           
--------------------------------------------------------------------------------
[0]                                      Tensor(3, 224, 224)  602.00 KB      
[1]                                      Tensor(1, 512)       2.00 KB        
[2]                                      str                  45 B           
[3]                                      dict                 161.24 MB      
  video_latent                           Tensor(81, 4, 60, 104) 161.24 MB   
  prompt                                 str                  128 B          
[4]                                      int                  28 B           
--------------------------------------------------------------------------------
```

## API Reference

### `analyze_pth_file(pth_file_path, max_depth=3, verbose=True)`

**Parameters:**
- `pth_file_path` (str): Path to the .pth file to analyze
- `max_depth` (int): Maximum depth for nested structure analysis (default: 3)
- `verbose` (bool): Whether to print detailed analysis (default: True)

**Returns:**
- `dict`: Analysis results containing:
  - `file_path`: Path to the analyzed file
  - `total_size_bytes`: Total size in bytes
  - `total_size_formatted`: Human-readable size
  - `num_variables`: Number of top-level variables
  - `variables`: OrderedDict with detailed information

### `get_size_bytes(obj)`

Calculate memory size of any Python object in bytes.

**Parameters:**
- `obj` (Any): Object to measure

**Returns:**
- `int`: Size in bytes

### `format_size(size_bytes)`

Format byte size into human-readable format.

**Parameters:**
- `size_bytes` (int): Size in bytes

**Returns:**
- `str`: Formatted size (e.g., "161.85 MB")

## Use Cases

### 1. Debug Data Processing

```python
# Check what's in a processed data file
results = analyze_pth_file('models/train/processed/sample.pth')

# Identify large tensors
for name, info in results['variables'].items():
    if info.get('size_bytes', 0) > 100 * 1024 * 1024:  # > 100MB
        print(f"Large tensor found: {name} - {info['size_formatted']}")
```

### 2. Optimize Storage

```python
# Compare sizes of different data formats
import glob

for pth_file in glob.glob('data/processed/*.pth'):
    results = analyze_pth_file(pth_file, verbose=False)
    print(f"{pth_file}: {results['total_size_formatted']}")
```

### 3. Validate Data Pipeline

```python
# Ensure all processed files have expected structure
expected_keys = ['video_latent', 'prompt', 'video_id']

results = analyze_pth_file('sample.pth', verbose=False)
for key in expected_keys:
    if key not in results['variables']:
        print(f"Warning: Missing expected key '{key}'")
```

## Understanding WanVideo Data Structure

The WanVideo data processing pipeline (`launch_data_process_task`) typically saves data in the following structure:

```python
{
    'video_latent': torch.Tensor,      # Encoded video latents
    'prompt': str,                      # Text prompt
    'video_id': str,                    # Unique identifier
    'video': torch.Tensor,              # Original video frames (optional)
    # ... other metadata
}
```

Use this analyzer to understand the exact structure and memory usage of your processed data.

## Troubleshooting

### Issue: "Failed to load file"

**Solution**: Ensure the file is a valid PyTorch pickle file and not corrupted.

```bash
# Verify file is readable
python -c "import torch; torch.load('file.pth')"
```

### Issue: Maximum recursion depth exceeded

**Solution**: Increase `max_depth` parameter:

```python
analyze_pth_file('file.pth', max_depth=10)
```

### Issue: Out of memory when loading large files

**Solution**: The analyzer loads files to CPU by default. For very large files:

```python
# Files are automatically loaded with map_location='cpu'
# But if still too large, you may need to process on a machine with more RAM
```

## Performance Tips

1. **Use appropriate `max_depth`**: Higher values provide more detail but take longer
2. **Disable verbose mode** for batch processing: `verbose=False`
3. **Process files in parallel** for multiple files:

```python
from multiprocessing import Pool

def analyze(path):
    return analyze_pth_file(path, verbose=False)

with Pool(8) as pool:
    results = pool.map(analyze, file_paths)
```

## Related Files

- `diffsynth/diffusion/runner.py`: `launch_data_process_task()` - Creates .pth files
- `diffsynth/core/data/unified_dataset.py`: `UnifiedDataset` - Loads .pth files
- `examples/analyze_pth_example.py`: Example usage script

## License

This tool is part of DiffSynth-Studio and follows the same license.
